{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Transition from R to Python\n",
    "\n",
    "After many failed attempts working with R's recosystem, I was able to get better results using Python's surprise library. One of the main issues using R's recosystem was that ratings would exceed the range of 1-5, while this library retained this constraint.\n",
    "\n",
    "## Submission 12\n",
    "\n",
    "I approached this with SVD, taking some of the better results from the R attempts. I used 20 latent factors, and a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from surprise import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = Dataset.load_from_file(\"train.txt\", reader=Reader(line_format='user item rating', sep=' '))\n",
    "test_set=pandas.read_csv(\"test.txt\", sep=' ', header=None, names=['id','user','book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set=data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "algo=SVD(n_factors=20,n_epochs=10,lr_all=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "algo.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings=[]\n",
    "\n",
    "for i in range(0,test_set.shape[0]):\n",
    "    ratings.append(algo.predict(str(test_set['user'][i]),test_set['book'][i])[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = pandas.DataFrame({\"id\": test_set['id'], \"rating\":ratings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.to_csv(\"py_output1.csv\",sep=\",\",index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 13\n",
    "\n",
    "Based on my findings before, I noticed the cost for items was smaller than the cost for users. In terms of bias, maybe this can translate to larger bias term in users. Rather than tuning (which takes a long time), I try to make an educated guess of better learning rate and regularization, increasing those for users, but decreasing it for items.\n",
    "\n",
    "In both python submissions, I noticed the minimum rating was around 1.2. This is something to adjust for, maybe applying a bigger cost parameter.\n",
    "\n",
    "This performed slightly worse. I think it may be also due to the fact that I reduced n_epochs to 10, which I should have kept at 15. However, I do not think changing n_epochs from 10 to 15 would change the output as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "algo=SVD(n_factors=20, n_epochs=10, lr_bu=0.02, reg_bu=0.05, lr_bi=0.001, reg_bi=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "algo.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings2=[]\n",
    "\n",
    "for i in range(0,test_set.shape[0]):\n",
    "    ratings2.append(algo.predict(str(test_set['user'][i]),test_set['book'][i])[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output2 = pandas.DataFrame({\"id\": test_set['id'], \"rating\":ratings2})\n",
    "output2.to_csv(\"py_output2.csv\",sep=\",\",index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min(ratings2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 14\n",
    "\n",
    "I will see if SVD++ will show if maybe the bias is less important than the implicit rating aspect. I will use the same parameters as submission 12 so I can make a useful comparison. I tried to use the same learning rate, but I do not think it converged (2 hours with no result). Unfortunately, this would require a couple of days to run and verify, so SVD is probably our best bet.\n",
    "\n",
    "I decided to double the number of latent factors from 20 to 40. This improved rmse by about 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algo=SVD(n_factors=40, n_epochs=10)\n",
    "algo.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings3=[]\n",
    "\n",
    "for i in range(0,test_set.shape[0]):\n",
    "    ratings3.append(algo.predict(str(test_set['user'][i]),test_set['book'][i])[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output3 = pandas.DataFrame({\"id\": test_set['id'], \"rating\":ratings3})\n",
    "output3.to_csv(\"py_output3.csv\",sep=\",\",index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 15\n",
    "\n",
    "I will test flooring ratings that are less than 1.5 to 1 to see if that improves the rmse. It barely performed any better or worse, so it is not a major issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings4=ratings3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(ratings4)):\n",
    "    if (ratings4[i]<1.5):\n",
    "        ratings4[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output4 = pandas.DataFrame({\"id\": test_set['id'], \"rating\":ratings4})\n",
    "output4.to_csv(\"py_output4.csv\",sep=\",\",index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 16\n",
    "\n",
    "In similar vein of submission 13, I will adjust the learning rate and regularization rate, but this time increasing on user and decreasing on item. I will follow submission 13's parameters of 40 factors. The minimum is 2.4, which I thought would translate to a worse rmse, but did slightly better again with another 0.01 deduction in the rmse. However, this may be overfitting, and I would go with submission 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algo=SVD(n_factors=40, n_epochs=10, lr_bu=0.001, reg_bu=0.01, lr_bi=0.03, reg_bi=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "algo.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings5=[]\n",
    "\n",
    "for i in range(0,test_set.shape[0]):\n",
    "    ratings5.append(algo.predict(str(test_set['user'][i]),test_set['book'][i])[3])\n",
    "\n",
    "print(min(ratings5),max(ratings5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output5 = pandas.DataFrame({\"id\": test_set['id'], \"rating\":ratings5})\n",
    "output5.to_csv(\"py_output5.csv\",sep=\",\",index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Submission 17 & 18\n",
    "\n",
    "I wanted to try a different model besides SVD. I attempted KNN, but the similarity matrix used up too much memory to construct for this package. Co-clustering seemed to only give me the mean rating on every test entry. I decided to take a step back and look at the baseline algorithm, which does not include the user-book interaction. Then I use SVD on the utility matrix (user-item matrix), using only 5 and 20 factors as an initial test. I got a comparable rmse to svd.\n",
    "\n",
    "I don't understand how to get lower rmse without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#algo=KNNWithMeans(k=400,min_k=10,sim_options={'name': 'pearson_baseline', 'user_based': True, 'min_support': 100, 'shrinkage':50})\n",
    "\n",
    "#algo=CoClustering(n_cltr_u=5, n_cltr_i=5, n_epochs=10,verbose=True)\n",
    "algo=BaselineOnly(bsl_options={'method':'sgd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using sgd...\n"
     ]
    }
   ],
   "source": [
    "algo.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26122590988 5\n"
     ]
    }
   ],
   "source": [
    "ratings6=[]\n",
    "\n",
    "for i in range(0,test_set.shape[0]):\n",
    "    ratings6.append(algo.predict(str(test_set['user'][i]),test_set['book'][i])[3])\n",
    "\n",
    "print(min(ratings6),max(ratings6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputbase = pandas.DataFrame({\"id\": test_set['id'], \"rating\":ratings6})\n",
    "outputbase.to_csv(\"py_outputbase.csv\",sep=\",\",index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "train_dat=pandas.read_csv(\"train.txt\", header=None, names=['user','book','rating'], sep=' ')\n",
    "train_dat\n",
    "ratingsmat=csr_matrix((train_dat['rating'],(train_dat['user'],train_dat['book'])),(train_dat.shape[0],train_dat.shape[0]))\n",
    "#from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20256439x20256439 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20256439 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ratingsmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/31523575/get-u-sigma-v-matrix-from-truncated-svd-in-scikit-learn\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "U, Sigma, VT = randomized_svd(ratingsmat, \n",
    "                              n_components=500,\n",
    "                              n_iter=1,\n",
    "                              random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0422788570491 0.166395263091\n"
     ]
    }
   ],
   "source": [
    "useriteminteract=[]\n",
    "\n",
    "for i in range(0,test_set.shape[0]):\n",
    "    useriteminteract.append(sum(U[test_set['user'][i],:] * Sigma * VT[:,test_set['book'][i]]))\n",
    "\n",
    "print(min(useriteminteract),max(useriteminteract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_ratings=pandas.read_csv(\"py_outputbase.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_ratings=pandas.DataFrame({\"id\": base_ratings['id'], \"rating\": base_ratings['rating'] + useriteminteract})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2612259098825829"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(new_ratings['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_ratings.to_csv(\"py_output7.csv\",sep=\",\",index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
